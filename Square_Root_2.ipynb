{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598c21c0",
   "metadata": {},
   "source": [
    "# The Square Root of 2\n",
    "\n",
    "Three root finding algorithms to compute the square root of 2, the bisection method, the Babylonian method, and Newton's method.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Bisection Method**  \\\n",
    "    a. Mathematical Basis  \\\n",
    "    b. Bisection Algorithm  \n",
    "2. **Fixed-Point Method**  \\\n",
    "    a. Mathematical Basis  \\\n",
    "    b. Choosing the Best $g$  \\\n",
    "    c. Fixed-Point Algorithm Based on the Babylonian Method $g$\n",
    "3. **Newton's Method**  \\\n",
    "    a. Mathematical Basis  \\\n",
    "    b. Newton's Method Algorithm  \n",
    "4. **Error Analysis**\n",
    "\n",
    "## The Bisection Method\n",
    "\n",
    "### The Mathematical Basis\n",
    "\n",
    "The **bisection algorithm** is based on the Intermediate Value Theorem (IVT) from calculus, \n",
    "\n",
    "* **IVT:** If $f\\in C([a,b])$, then every intermediate $y$-value $k$ between $f(a)$ and $f(b)$ is achieved: there exists a solution $x=c\\in (a,b)$ of the equation $f(x)=k$.\n",
    "\n",
    "To use this theorem, we must choose a function $f$ which is continuous on a compact interval $[a,b]$ containing $x=\\sqrt{2}$, the solution to $f(x)=0$.  From the definition of $\\sqrt{2}$ we know \n",
    "\n",
    "$$\n",
    "(\\sqrt{2})^2=2\\ \\Longleftrightarrow\\ (\\sqrt{2})^2-2=0\n",
    "$$ \n",
    "\n",
    "which demonstrates that the polynomial \n",
    "\n",
    "$$\n",
    "f(x)=x^2-2\n",
    "$$ \n",
    "\n",
    "has $x=\\sqrt{2}$ as such a solution. Lastly, we need $[a,b]$ with $f(a)<0<f(b)$ or $f(a)>0>f(b)$.  Clearly $[1,2]$ serves this purpose.  \n",
    "\n",
    "### The Bisection Algorithm\n",
    "* **Input:**  $f(x)=x^2-2$, $a=1$ and $b=2$.\n",
    "\n",
    "    1. **Set error, tolerance, and counter:** $0<e=$ error size, $N=$ number of iterations, $n=0$ counter initial value.\n",
    "    2. **Set initial values, and their midpoint bisector:** $a_1=a$, $b_1=b$ and $c_1=(a_1+b_1)/2$.\n",
    "    3. **Test $c_1$** to make sure we haven't accidentally hit upon the solution.\n",
    "        + If $f(c_1)=0$, then $x=c_1$ is our solution. We are done. (Very unlikely, as $\\sqrt{2}$ is irrational, but good to check generally).\n",
    "        + If $|f(c_1)|<e$, we have satisfied our error tolerance, and $x=c_1$ is an approximate solution. We are done. \n",
    "        + If $n=N$, we stop without reaching a good enough approximation.\n",
    "        + Else, set $n=n+1$ and proceed to the next step.  \n",
    "    4. **Test for the sign of $f(c_1)$:** Neither $f(a_1)$, nor $f(b_1)$ nor $f(c_1)$ equal $0$. \n",
    "        + If $f(c_1)f(a_1)<0$, then the solution lies in $(a_1,c_1)$ by IVT.  Therefore set \n",
    "            * $a_2=a_1$, $b_2=c_1$, $c_2=\\frac{a_2+b_2}{2}$.  (The solution lies in $[a_2,b_2]=[a_1,c_1]$, and now we will test its midpoint $c_2$)\n",
    "            * Repeat steps 3 and 4 with $a_2$, $b_2$ and $c_2$ in place of $a_1$, $b_1$, $c_1$.\n",
    "        + Else, the solution lies in $(c_1,b_1)$ by IVT.  Therefore set \n",
    "            * $a_2=c_1$, $b_2=b_1$, $c_2=\\frac{a_2+b_2}{2}$.\n",
    "            * Repeat steps 3 and 4 with $a_2$, $b_2$ and $c_2$ in place of $a_1$, $b_1$, $c_1$.\n",
    "    5. etc.\n",
    "\n",
    "## The Fixed Point Method\n",
    "\n",
    "### The Mathematical Basis\n",
    "\n",
    "The **fixed point method** requires $g\\in C([a,b])$, as before, *and also* $g([a,b])\\subseteq [a,b]$, because we are interested in finding a **fixed point** $p\\in [a,b]$, satisfying $g(p)=p$.  The IVT can be use to show that such a $g$ has at least one fixed point:  If $g(a)=a$ or $g(b)=b$, we are done, we have a fixed point.  Otherwise, $a<g(a)$ and $g(b)<b$, in which case define \n",
    "\n",
    "$$\n",
    "h(x)=g(x)-x\n",
    "$$ \n",
    "\n",
    "and observe that $h(b)<0<h(a)$.  IVT says $\\exists p\\in (a,b)$ with $h(p)=0$.  But $h(p)=0$ means $g(p)=p$.  \n",
    "\n",
    "If, *thirdly*, $\\sup |g'|<1$ on $[a,b]$ (making $g$ what is called a **contraction**), then the Mean Value Theorem (MVT) implies uniqueness for the fixed point. (Exercise!)  The **Banach Fixed-Point Theorem** is the name given to this proposition.  An algorithm is got by starting with any initial guess $p_0$ and then *plugging* $p_n$ *back into* $g$ to get $p_{n+1}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p_0\\in [a,b]\\\\\n",
    "&p_{n+1}=g(p_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The MVT guarantees convergence $p_n\\to p$ to the unique fixed point of $g$.  The algorithm is the essence of simplicity, but depends heavily on the choice of contraction $g$.  If $g$ is not a contraction, the sequence $(p_n)_{n\\in \\mathbb{N}}$ may not converge.\n",
    "\n",
    "### Choosing the best $g$ for $\\sqrt{2}$\n",
    "\n",
    "#### Choice \\#1\n",
    "\n",
    "The following simple observation may be of potential help.\n",
    "\n",
    "* ***Proposition*** $f\\in C([a,b])$ has a root $f(p)=0$ in $[a,b]$ iff the associated function $g\\in C([a,b])$ given by $g(x)=x-f(x)$ has a fixed point at $p$.\n",
    "\n",
    "Now, $\\sqrt{2}$ is a root of the polynomial $f(x) = x^2-2$, so our first choice of $g$ will be $f$'s associated fixed point function $g(x) = x-f(x) = x-x^2+2$.  Note that $g\\in C([0,2])$ and an easy calculation shows that $g([0,2])\\subseteq [0,2]$.  Since $0<\\sqrt{2}<2$, $g$ will hit this value by IVT, and this is the fixed point of $g$ associated to the root of $f$. But $g$ is *not a contraction* (in fact $\\sup |g'|=3.5$ on $[0,2]$).  \n",
    "\n",
    "#### Choice \\#2\n",
    "\n",
    "A similar problem occurs with $g(x) = 2/x$ on $[1,2]$. (Exercise!)\n",
    "\n",
    "#### Choice \\#3: the Babylonian Method\n",
    "\n",
    "The solution is the **Babylonian method**, which uses $g\\in C([1,2])$ given by $g(x) = (x+2/x)/2$: this function satisfies $g([1,2])=[\\sqrt{2}, 3/2] \\subseteq [1,2]$, showing that it has a fixed point, and this fixed point is $\\sqrt{2}$ since $p=g(p)$ means $p=(p+2/p)/2$, which simplifies to $p^2=2$. Moreover, $g$ is a contraction satisfying $|g'(x)| \\leq 1/2 < 1$, so the Banach Fixed Point Theorem also implies uniquness of $p$ and convergence of $(p_n)_{n\\in\\mathbb{N}}$.  \n",
    "\n",
    "### The Fixed Point Algorithm\n",
    "\n",
    "* **Input:**  $\\displaystyle g(x)=\\frac{1}{2}\\Bigl(x+\\frac{2}{x}\\Bigr)$, $a=1$ and $b=2$.\n",
    "\n",
    "    1. **Set error, tolerance, and counter:** $0<e=$ error size, $N=$ number of iterations, $n=0$ counter initial value.\n",
    "    2. **Choose initial estimate:** Choose $p_0\\in [1,2]$.\n",
    "    3. **Recursion (Babylonian method):** Let $\\displaystyle p_1=g(p_0)=\\frac{1}{2}\\Bigl(p_0+\\frac{2}{p_0}\\Bigr)$\n",
    "        + If $|p_2-p_1|<e$, stop.  We have a good enough approximation in $p_2$. \n",
    "        + If $n=N$, we stop without reaching a good enough approximation.\n",
    "        + Else, set $n=n+1$ and repeat this step, iii.  \n",
    "\n",
    "## Newton's Method\n",
    "\n",
    "### The Mathematical Basis\n",
    "\n",
    "Suppose we know that $f\\in C^2([a,b])$ has a root $p\\in [a,b]$ (for example by observing $f(a)f(b)<0$ and applying IVT). Take an initial guess $p_0\\approx p$ in $[a,b]$, and expand $f$ into a linear Taylor polynomial about $x=p_0$,\n",
    "\n",
    "$$\n",
    "\\displaystyle 0=f(p)\\approx f(p_0)+f'(p_0)(p-p_0)\n",
    "\\ \\implies\\ p\\approx p_0-\\frac{f(p_0)}{f'(p_0)}\n",
    "$$\n",
    "\n",
    "Needless to say, we must require $f'(p)\\neq 0$, since then $f'(x)\\neq 0$ in a small neighborhood $\\overline{V_\\delta(p)}=[p-\\delta,p+\\delta]$ of $p$, because $f'\\in C^1([a,b])$.  The function \n",
    "\n",
    "$$\n",
    "g(x)=x-\\frac{f(x)}{f'(x)}\n",
    "$$\n",
    "\n",
    "is then continuous on $\\overline{V_\\delta(p)}$.  Its derivative\n",
    "\n",
    "$$\n",
    "g'(x)=\\frac{f(x)f\"(x)}{f'(x)^2}\n",
    "$$\n",
    "\n",
    "is continuous on $\\overline{V_\\delta(p)}$ and satisfies $g'(p)=0$.  Shrinking $\\delta$ as needed, we can make sure that $\\sup |g'|\\leq k<1$ on $\\overline{V_\\delta(p)}$, and then use the Banach Fixed Point Theorem to guarantee convergence of the recursive sequence \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p_0\\in [a,b]\\\\\n",
    "&p_{n+1}=g(p_n)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In our case, \n",
    "\n",
    "$$\n",
    "g(x)=x-\\frac{f(x)}{f'(x)}=\\frac{x}{2}+\\frac{1}{x}\\ \\implies\\ g'(x)=\\frac{1}{2}-\\frac{1}{x^2}\n",
    "$$ \n",
    "\n",
    "Moreover, $g'$ is increasing on $[1,2]$ because $g\"(x)=\\frac{2}{x^3}>0$ there, while $g'(1)=-\\frac{1}{2}$ and $g'(2)=\\frac{1}{4}$, which shows that $\\sup |g'|=\\frac{1}{2}<1$.  Our $g$ is a contraction on $[1,2]$, hence it has a unique fixed point, $p=g(p)=\\frac{p}{2}+\\frac{1}{p}$, which is equivalent to $p^2=2$.\n",
    "\n",
    "### Newton's Method Algorithm\n",
    "\n",
    "* **Input:**  $\\displaystyle g(x)=\\frac{x}{2}+\\frac{1}{x}$, $a=1$ and $b=2$.\n",
    "\n",
    "    1. **Set error, tolerance, and counter:** $0<e=$ error size, $N=$ number of iterations, $n=0$ counter initial value.\n",
    "    2. **Choose initial estimate:** Choose $p_0\\in [1,2]$.\n",
    "    3. **Recursion (Newton's method):** Let $\\displaystyle p_1=g(p_0)=\\frac{p_0}{2}+\\frac{1}{p_0}$\n",
    "        + If $|p_2-p_1|<e$, stop.  We have a good enough approximation in $p_2$. \n",
    "        + If $n=N$, we stop without reaching a good enough approximation.\n",
    "        + Else, set $n=n+1$ and repeat this step, iii.  \n",
    "\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "Let $(p_n)_{n\\in \\mathbb{N}}$ be a convergent sequence, $p_n\\to p$ satisfying \n",
    "* $p_n\\neq p$ for all $n\\in\\mathbb{N}$\n",
    "* $\\lim_{n\\to\\infty}\\frac{|p_{n+1}-p|}{|p_n-p|^\\alpha}=\\lambda>0$, $\\alpha>0$ ($p_n$ **converges to $p$ of order $\\alpha$**)\n",
    "\n",
    "Comparison of the three methods shows that the bisection algorithm takes much longer to converge than the Babylonian and Newton algorithms.  This is because the bisection algorithm can be shown to converge only *linearly* ($\\alpha=1$), whereas the Babylonian and Newton algorithms converge *quadratically* ($\\alpha=2$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3e8ef",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9de2417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t       The Square Root of 2\n",
      "\n",
      "\t\t\t     Comparison of the Methods\n",
      "\n",
      "\t-----------------------------------------------------------------------\n",
      "\t  n  \t    Bisection   \t  Babylonian    \t  Newton's\n",
      "\t-----------------------------------------------------------------------\n",
      "\n",
      "\t  1 \t   1.5000000000\t \t 1.0000000000 \t\t 1.0000000000\n",
      "\t  2 \t   1.2500000000\t \t 1.5000000000 \t\t 1.5000000000\n",
      "\t  3 \t   1.3750000000\t \t 1.4166666667 \t\t 1.4166666667\n",
      "\t  4 \t   1.4375000000\t \t 1.4142156863 \t\t 1.4142156863\n",
      "\t  5 \t   1.4062500000\t \t 1.4142135624 \t\t 1.4142135624\n",
      "\t  6 \t   1.4218750000\t \t 1.4142135624 \t\t 1.4142135624\n",
      "\t  7 \t   1.4140625000\t \t  \t\t \n",
      "\t  8 \t   1.4179687500\t \t  \t\t \n",
      "\t  9 \t   1.4160156250\t \t  \t\t \n",
      "\t  10 \t   1.4150390625\t \t  \t\t \n",
      "\t  11 \t   1.4145507812\t \t  \t\t \n",
      "\t  12 \t   1.4143066406\t \t  \t\t \n",
      "\t  13 \t   1.4141845703\t \t  \t\t \n",
      "\t  14 \t   1.4142456055\t \t  \t\t \n",
      "\t  15 \t   1.4142150879\t \t  \t\t \n",
      "\t  16 \t   1.4141998291\t \t  \t\t \n",
      "\t  17 \t   1.4142074585\t \t  \t\t \n",
      "\t  18 \t   1.4142112732\t \t  \t\t \n",
      "\t  19 \t   1.4142131805\t \t  \t\t \n",
      "\t  20 \t   1.4142141342\t \t  \t\t \n",
      "\t  21 \t   1.4142136574\t \t  \t\t \n",
      "\t  22 \t   1.4142134190\t \t  \t\t \n",
      "\t  23 \t   1.4142135382\t \t  \t\t \n",
      "\t  24 \t   1.4142135978\t \t  \t\t \n",
      "\t  25 \t   1.4142135680\t \t  \t\t \n",
      "\t  26 \t   1.4142135531\t \t  \t\t \n",
      "\t  27 \t   1.4142135605\t \t  \t\t \n",
      "\t  28 \t   1.4142135642\t \t  \t\t \n",
      "\t  29 \t   1.4142135624\t \t  \t\t \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\t\\t\\t       The Square Root of 2\")\n",
    "print(\"\\n\\t\\t\\t     Comparison of the Methods\\n\")\n",
    "print(\"\\t-----------------------------------------------------------------------\")\n",
    "print(\"\\t  n  \\t    Bisection   \\t  Babylonian    \\t  Newton's\")\n",
    "print(\"\\t-----------------------------------------------------------------------\\n\")\n",
    "    \n",
    "###########################\n",
    "#  Bisection Method\n",
    "\n",
    "n=0\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "\n",
    "e = 1e-10\n",
    "N = 41\n",
    "k = 0\n",
    "\n",
    "BIS = []\n",
    "\n",
    "for i in range(1,N):\n",
    "    n = n+1\n",
    "    c = (a+b)/2\n",
    "    y1 = a**2-2\n",
    "    y2 = b**2-2\n",
    "    y = c**2-2\n",
    "    BIS.append(c)\n",
    "    \n",
    "    if y == 0 or abs(y)<e:\n",
    "        k = 1\n",
    "        break\n",
    "    elif y*y1<0:\n",
    "        b = c\n",
    "    else:\n",
    "        a = c\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "#  Babylonian Method + Banach Fixed Point Theorem\n",
    "\n",
    "p = 1           # Initial guess, p_0\n",
    "e = 1e-10       # Tolerance: 10 decimals\n",
    "n = 0           # Counter (for # steps)\n",
    "m = 40          # Upper limit on 'for' loop\n",
    "\n",
    "BAB = []\n",
    "\n",
    "for i in range(1,m+1):\n",
    "    n = n+1\n",
    "    BAB.append(p)\n",
    "    q = p\n",
    "    p = (p+2/p)/2\n",
    "    if abs(q-p)<e:\n",
    "        BAB.append(p)\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "#  Newton's Method\n",
    "\n",
    "p=1     # initial guess\n",
    "q=p     \n",
    "n=0     # counter\n",
    "e=1e-10 # tolerance / error\n",
    "N = 41  # upper limit on number of iterations\n",
    "\n",
    "NEWT = []\n",
    "\n",
    "for i in range(1,N):\n",
    "    NEWT.append(p)\n",
    "    p = p/2+1/p\n",
    "    if abs(p-q)<e:\n",
    "        NEWT.append(p)\n",
    "        break\n",
    "    q = p\n",
    "    n = n+1\n",
    "\n",
    "\n",
    "###########################\n",
    "r = [len(BIS),len(BAB),len(NEWT)]    \n",
    "\n",
    "m = max(r)\n",
    "d = 0\n",
    "D = []\n",
    "\n",
    "for i in range(m+1):\n",
    "    d=d+1\n",
    "    D.append(d)\n",
    "\n",
    "t = [D,BIS,BAB,NEWT]\n",
    "\n",
    "if len(BIS)<m:\n",
    "    l = len(BIS)\n",
    "    for j in range(l+1,m+1):\n",
    "        BIS.append(0)\n",
    "        \n",
    "if len(BAB)<m:\n",
    "    l = len(BAB)\n",
    "    for j in range(l+1,m+1):\n",
    "        BAB.append(0)\n",
    "        \n",
    "if len(NEWT)<m:\n",
    "    l = len(NEWT)\n",
    "    for j in range(l+1,m+1):\n",
    "        NEWT.append(0)\n",
    "\n",
    "for s, x, y, z in zip(D,BIS,BAB,NEWT):\n",
    "    print(\"\\t  %d\" % s,\"\\t   %.10f\\t\" % x,\"\\t\",format(y, \".10f\") if y else \"\",\\\n",
    "          \"\\t\\t\",format(z, \".10f\") if z else \"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
